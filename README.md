# Spotify-Top-Songs
Spotify is a multimedia platform used worldwide which gives access to different content like songs, podcasts etc from all over the world. Most of the time we all wonder why we enjoy a particular song or how certain songs become popular over others. To find these answers we tried to come up with a solution after analyzing Spotify's dataset. We followed a few steps to analyze data like going through the data, Visualising it using different Python libraries, running regression models to get the most relevant features and then selecting and fine-tuning a model. We continued with the Decision Tree and Random Forest Regressor. We choose the model with the lowest RMSE score as the most representative.

## Dataset Analysis
Dataset Analysis The train dataset consists of 453 instances and 15 columns, 3 of which are object types (title, artist, top genre).

In our initial look, I want to address a few concerns. I wanted to uncover any issues with this dataset as soon as possible so that we could correct them in our data pre-processing. So, we checked if there were any null values in the dataset and we found there were 15 in the 'top genre' column only which we can remove in the prediction phase. When we searched for duplicates there was one which would not affect our analysis it was not removed. At last, we dropped the 'id' column as it was not valuable to us.

## Data Preparation
1. Stratified Sampling We added a new feature to data like song age which how old the song is and then we decided to drop columns such as 'Id',' artist', 'title', and 'top genre' because they have are not of analytical importance ('Id) or are of categorical nature ('artist', 'title', 'top genre'). OneHot Encoding would have created so many extra columns, making it difficult to manage. Also, as we noticed during the data exploration, 'top genre' had 15 missing values, so by dropping the column we are solving this issue. At last, we fill the null values with 'adult standards'. In our original attempt, we included all the remaining columns, but we noticed that attributes with very low correlation to 'pop' such as val, live, and bpm didn't add anything to the model, therefore decided to go with a simple model and add features if needed.
2. Stratified sampling is necessary when some of the features are skewed. We want to make sure that our data are representative of the whole population. 'Acous' seems to be skewed and also has the highest correlation (-0.466537). Therefore, we decided to introduce a new column in the data frame and group the data into categories. We use this column to inform the new train/test split.

## Regression Analysis
For actual regression analysis, we have taken 3 regression methods. We started with a simple linear regression model, then moved to a Decision Tree, and finally employed a random forest regression model. We found that the random forest model gave us the best results in Kaggle, with a public score of 8.19.

To begin with, we started by splitting the training data into testing and training data to validate our findings before fitting the regression model to the whole training data. We split the training data into 70:30 as training and validation data.

We first explored a linear regression model that models a plain linear relationship between the explanatory and the predicted variable. The purpose was to have a baseline model. Then, we employed a random forest model. It is a decision-making tool which makes it possible to represent decisions and all of their potential consequences, including outcomes, input costs, and utility. Here dataset is broken into smaller subsets and by the side decision tree is developed. We calculate the mean square error and try to minimise it with every node. At last, to get the best results we applied Random Forest Regression. Random forest regression is an ensemble learning, which combines multiple decision trees for its output. The idea behind choosing it was to limit the possible overfitness of the data and also to reduce its sensitivity to noise. As we saw earlier, there are unaddressed outliers in the data which might be affecting the performance of the regression analysis.
Once we trained our model we had a high value of RMSE and index errors were shown when removing the null value and the outliers so we came up with categorizing the null values. Random forest was the accurate model to predict the song popularity and by using XGboost and voting regressor our RMSE decreased it was not as low as expected it might be due to many reasons like overriding of data etc. Our model will perform better as we train it and a few amendments in data exploration can lead to better predictions as we tried and got a score from Kaggle which was 7.58 better than the previous one.

## Conclusion
Using highly connected features for popularity prediction can help in enhancing the model output. In addition, to improve the performance we can do additional data cleansing and can use one hot encoder or dummy for encoding the character variables. It's important to note that the accuracy of the model will depend on the quality of the dataset and the features used for prediction. Additionally, the popularity of a song is a complex phenomenon that is influenced by many factors, so the model may not be able to capture all of the nuances that affect a song's popularity. Nonetheless, random forest regression can be a useful tool for predicting the popularity of songs to a certain extent.
